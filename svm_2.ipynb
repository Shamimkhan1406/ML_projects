{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc62c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28130d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = pd.read_csv('emotion_dataset_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ee8365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14929</td>\n",
       "      <td>anger</td>\n",
       "      <td>@charlottehanlan i agree, angry birds is exact...</td>\n",
       "      <td>agree angry birds exactly described</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21962</td>\n",
       "      <td>anger</td>\n",
       "      <td>CRAP! Didn't think I'd have to know and unders...</td>\n",
       "      <td>CRAP think Id know understand statistics again...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15735</td>\n",
       "      <td>anger</td>\n",
       "      <td>Don ’ t criticize Mario or else I ’ ll start ...</td>\n",
       "      <td>’ criticize Mario ’ start fat  drunk friends  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18326</td>\n",
       "      <td>anger</td>\n",
       "      <td>Are you done ?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29278</td>\n",
       "      <td>anger</td>\n",
       "      <td>He was coming so fast that she knew he had not...</td>\n",
       "      <td>coming fast knew seen brake sharply  frowning ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Emotion                                               Text  \\\n",
       "0       14929   anger  @charlottehanlan i agree, angry birds is exact...   \n",
       "1       21962   anger  CRAP! Didn't think I'd have to know and unders...   \n",
       "2       15735   anger   Don ’ t criticize Mario or else I ’ ll start ...   \n",
       "3       18326   anger                                    Are you done ?    \n",
       "4       29278   anger  He was coming so fast that she knew he had not...   \n",
       "\n",
       "                                          Clean_Text  \n",
       "0                agree angry birds exactly described  \n",
       "1  CRAP think Id know understand statistics again...  \n",
       "2  ’ criticize Mario ’ start fat  drunk friends  ...  \n",
       "3                                                NaN  \n",
       "4  coming fast knew seen brake sharply  frowning ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed08aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13524, 4)\n"
     ]
    }
   ],
   "source": [
    "print(emotion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed7a0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions are:  ['anger' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "print('Emotions are: ', emotion['Emotion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66507d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "Emotion         0\n",
       "Text            0\n",
       "Clean_Text    452\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af451bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184c21a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "anger       2254\n",
       "fear        2254\n",
       "joy         2254\n",
       "neutral     2254\n",
       "sadness     2254\n",
       "surprise    2254\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5fc113a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shamimkhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import spacy\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85cba2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion['Clean_Text'] = emotion['Clean_Text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25aa211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "  bs = BeautifulSoup(text, 'html.parser')\n",
    "  return ' ' + bs.get_text() + ' '\n",
    "\n",
    "def keep_only_letters(text):\n",
    "  text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "  return text\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "  return text.lower()\n",
    "\n",
    "def clean_reviews(text):\n",
    "  text = remove_html(text)\n",
    "  text = keep_only_letters(text)\n",
    "  text = convert_to_lowercase(text)\n",
    "  return text\n",
    "\n",
    "\n",
    "emotion['Clean_Text'] = emotion['Clean_Text'].apply(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed4724dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of English stopwords: 198\n",
      "\n",
      "First 20 stopwords:\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
     ]
    }
   ],
   "source": [
    "english_stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print(f\"Total number of English stopwords: {len(english_stop_words)}\")\n",
    "print(\"\\nFirst 20 stopwords:\")\n",
    "print(english_stop_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bfba26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "  for stopword in english_stop_words:\n",
    "    stopword = ' ' + stopword + ' '\n",
    "    text = text.replace(stopword, ' ')\n",
    "    return text\n",
    "\n",
    "emotion['Clean_Text'] = emotion['Clean_Text'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "123fb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stemming(text):\n",
    "  stemmer = nltk.porter.PorterStemmer()\n",
    "  stemmed = ' '.join([stemmer.stem(token) for token in text.split()])\n",
    "  return stemmed\n",
    "\n",
    "emotion['Text'] = emotion['Text'].apply(text_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea2d9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = emotion.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42d306f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10819,)\n",
      "y_train shape: (10819,)\n",
      "X_test shape: (2705,)\n",
      "y_test shape: (2705,)\n",
      "\n",
      "Training set emotion distribution:\n",
      "Emotion\n",
      "anger       0.166744\n",
      "neutral     0.166651\n",
      "fear        0.166651\n",
      "sadness     0.166651\n",
      "joy         0.166651\n",
      "surprise    0.166651\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Define your features (X) and target (y)\n",
    "# X will be the 'Clean_Text' column you processed\n",
    "# y will be the 'Emotion' column\n",
    "X = emotion['Clean_Text']\n",
    "y = emotion['Emotion']\n",
    "\n",
    "# 2. Split the data into training and testing sets\n",
    "# We use test_size=0.2 for a standard 80% train / 20% test split\n",
    "# random_state=42 ensures you get the same split every time you run this (for reproducible results)\n",
    "# stratify=y is very important here. Since your dataset is balanced, this ensures \n",
    "#            that both the training and testing sets have the same proportion of emotions.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)\n",
    "\n",
    "# 3. (Optional) Check the shapes of your new datasets to confirm\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# 4. (Optional) Check the distribution in the training set\n",
    "print(\"\\nTraining set emotion distribution:\")\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cc0f7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tfidf shape: (10819, 5000)\n",
      "X_test_tfidf shape: (2705, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Initialize the TfidfVectorizer\n",
    "# We can set max_features to limit the vocabulary to the top N most frequent words,\n",
    "# which helps control the dimensionality and remove very rare words.\n",
    "# 5000 is a good starting point.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "\n",
    "# 2. Fit the vectorizer on the training data and transform it\n",
    "# This learns the vocabulary (the \"fit\") and converts X_train to a TF-IDF matrix (the \"transform\")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# 3. Transform the test data using the *same* fitted vectorizer\n",
    "# This only does the \"transform\" step, using the vocabulary learned from X_train.\n",
    "# This is crucial to prevent data leakage.\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# 4. (Optional) Check the new shapes\n",
    "# The second dimension (the 5000) should match.\n",
    "print(\"X_train_tfidf shape:\", X_train_tfidf.shape)\n",
    "print(\"X_test_tfidf shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c569c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SVC model with polynomial kernel...\n",
      "Training the model...\n",
      "Training completed in 5.07 seconds.\n",
      "\n",
      "Making predictions on the test set...\n",
      "\n",
      "Model Accuracy: 57.52%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.58      0.55      0.56       450\n",
      "       anger       0.68      0.57      0.62       451\n",
      "    surprise       0.50      0.54      0.52       451\n",
      "     neutral       0.62      0.80      0.70       451\n",
      "        fear       0.53      0.50      0.51       451\n",
      "         joy       0.55      0.49      0.52       451\n",
      "\n",
      "    accuracy                           0.58      2705\n",
      "   macro avg       0.58      0.58      0.57      2705\n",
      "weighted avg       0.58      0.58      0.57      2705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "\n",
    "# 1. Initialize the SVC model with a polynomial kernel\n",
    "# kernel='poly': Specifies the polynomial kernel\n",
    "# degree=3: A common starting point for the polynomial degree\n",
    "# C=1.0: The regularization parameter.\n",
    "print(\"Initializing SVC model with polynomial kernel...\")\n",
    "svc_poly = SVC(kernel='linear', degree=3, C=1.0)\n",
    "\n",
    "\n",
    "# 2. Train the model\n",
    "# This will take a moment, as SVC can be computationally intensive\n",
    "print(\"Training the model...\")\n",
    "start_time = time.time() # Start timer\n",
    "\n",
    "svc_poly.fit(X_train_tfidf, y_train)\n",
    "\n",
    "end_time = time.time() # End timer\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# 3. Make predictions on the test set\n",
    "print(\"\\nMaking predictions on the test set...\")\n",
    "y_pred = svc_poly.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "# 4. Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "# This report gives you precision, recall, and f1-score for each emotion\n",
    "print(classification_report(y_test, y_pred, target_names=emotion['Emotion'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d1a32a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold cross-validation...\n",
      "This will train and test the model 5 times, so it may take a few moments...\n",
      "Cross-validation completed in 8.22 seconds.\n",
      "\n",
      "--- Cross-Validation Results ---\n",
      "Accuracy for each of the 5 folds:\n",
      "  Fold 1: 57.07%\n",
      "  Fold 2: 56.70%\n",
      "  Fold 3: 57.53%\n",
      "  Fold 4: 57.16%\n",
      "  Fold 5: 57.37%\n",
      "\n",
      "Average Accuracy:\n",
      "  57.17%\n",
      "\n",
      "Standard Deviation:\n",
      "  0.28%\n",
      "---------------------------------\n",
      "\n",
      "(This average accuracy is a more reliable estimate of your model's performance.)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC # Make sure to import SVC\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 1. Re-initialize the *exact same* model you used before.\n",
    "#    We create a fresh, untrained model for the cross-validation process.\n",
    "#    Based on your last cell, you used SVC(kernel='linear').\n",
    "model_for_cv = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# 2. Get the full training dataset\n",
    "#    We use the complete X_train_tfidf and y_train.\n",
    "#    cross_val_score will handle splitting it into 5 folds automatically.\n",
    "X_train_full_tfidf = X_train_tfidf\n",
    "y_train_full = y_train\n",
    "\n",
    "print(\"Starting 5-fold cross-validation...\")\n",
    "print(\"This will train and test the model 5 times, so it may take a few moments...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 3. Run the cross-validation\n",
    "#    cv=5 tells it to perform 5-fold cross-validation.\n",
    "#    'accuracy' is the metric we want to score.\n",
    "#    n_jobs=-1 (optional) speeds up the process by using all available CPU cores.\n",
    "scores = cross_val_score(model_for_cv, \n",
    "                         X_train_full_tfidf, \n",
    "                         y_train_full, \n",
    "                         cv=5, \n",
    "                         scoring='accuracy',\n",
    "                         n_jobs=-1) # Use -1 to use all processors\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Cross-validation completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# 4. Print the results\n",
    "print(\"\\n--- Cross-Validation Results ---\")\n",
    "print(f\"Accuracy for each of the 5 folds:\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"  Fold {i+1}: {score * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nAverage Accuracy:\")\n",
    "print(f\"  {np.mean(scores) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nStandard Deviation:\")\n",
    "print(f\"  {np.std(scores) * 100:.2f}%\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"\\n(This average accuracy is a more reliable estimate of your model's performance.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f50372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cad1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645d04c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be320936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
